{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output, display\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def extract_urls_from_sitemap(file_path):\n",
    "    urls = []\n",
    "\n",
    "    # Parse the XML file\n",
    "    tree = ET.parse(file_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # Iterate through the 'url' elements and extract the 'loc' text\n",
    "    for url_element in root.findall('{http://www.sitemaps.org/schemas/sitemap/0.9}url'):\n",
    "        loc_element = url_element.find('{http://www.sitemaps.org/schemas/sitemap/0.9}loc')\n",
    "        if loc_element is not None:\n",
    "            urls.append(loc_element.text)\n",
    "\n",
    "    return urls\n",
    "\n",
    "sitemap_file_path = './data/goodreads_genres.xml'\n",
    "urls = extract_urls_from_sitemap(sitemap_file_path)\n",
    "for url in urls[:4]:\n",
    "    print(url)\n",
    "    \n",
    "print(len(urls))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.goodreads.com/genres/fiction -> https://www.goodreads.com/genres/new_releases/fiction\n",
    "def new_releases(s: str) -> str:\n",
    "    genre = s.split(\"/\")[-1]\n",
    "    genre_len = len(genre)\n",
    "    return s[:-genre_len]+\"new_releases/\"+genre\n",
    "\n",
    "print(new_releases(\"https://www.goodreads.com/genres/fiction\"))\n",
    "\n",
    "# After we've gotten all the \"new_release\" it would be nice to go get the \"most_read\"\n",
    "# https://www.goodreads.com/genres/classics -> https://www.goodreads.com/genres/most_read/classics\n",
    "def most_read(s: str) -> str:\n",
    "    genre = s.split(\"/\")[-1]\n",
    "    genre_len = len(genre)\n",
    "    return s[:-genre_len]+\"most_read/\"+genre\n",
    "\n",
    "print(most_read(\"https://www.goodreads.com/genres/classics\"))\n",
    "\n",
    "def write(books):\n",
    "    df = pd.DataFrame(books, columns=[\"URL_ID\", \"Author\", \"Title\", \"Genres\", \"Blurb\", \"Average Rating\", \"My Rating\"])\n",
    "    print(df)\n",
    "    df.to_csv('./data/book_data.csv', index=False)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_book_data(url):\n",
    "    time.sleep(0.1)\n",
    "    book_page = requests.get(url) # We assume this works because otherwise there'd be a broken, clickable link.\n",
    "    sub_soup = BeautifulSoup(book_page.content, 'html.parser')\n",
    "    \n",
    "    # Extract the author -- Sometimes the author isn't present-- not totally sure why. Maybe a load problem.\n",
    "    # We might pick it up on a future run, don't worry too much about it for now.\n",
    "    try:\n",
    "        author = sub_soup.find('span', {'class':'ContributorLink__name', 'data-testid':'name'}).text\n",
    "    except:\n",
    "        display(f\"Trouble finding author for {url}; skipping for now...\")\n",
    "        return None\n",
    "    # Extract the title\n",
    "    title_element = sub_soup.find('meta', {'property': 'og:title'})\n",
    "    title = title_element['content']\n",
    "    \n",
    "    # Extract the Blurb\n",
    "    blurb_div = sub_soup.find('div', {'data-testid': 'description'})\n",
    "    blurb_span = blurb_div.find('span', {'class': 'Formatted'})\n",
    "    blurb = blurb_span.text\n",
    "\n",
    "    # Extract Genres\n",
    "    genres_div = sub_soup.find('div', {'data-testid': 'genresList'})\n",
    "    if not genres_div:\n",
    "        display(f\"Trouble finding genres for {url}; skipping for now...\")\n",
    "        return None\n",
    "    genres_span = genres_div.find_all('span', {'class': 'BookPageMetadataSection__genreButton'})\n",
    "    genres = []\n",
    "    for s in genres_span:\n",
    "        genres.append(s.find('span', {'class': 'Button__labelItem'}).text)\n",
    "        \n",
    "    # Extract Rating (out of 500 becaues I like integers)\n",
    "    rating = int(float(sub_soup.find('div', {'class': 'RatingStatistics__rating'}).text)*100)\n",
    "    \n",
    "    # Short summary (missing URL_ID and blurb)\n",
    "    # display(f\"Author: {author},\\n Title: {title},\\n Genres: {genres},\\n Rating: {rating}\\n\\n\")\n",
    "    return [author, title, \" \".join(genres), blurb, rating]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr_prefix = \"https://www.goodreads.com/book/show/\"\n",
    "\n",
    "# Goal dataset should be something like\n",
    "# URL_ID - Author - Genre - Blurb - Rating - My Rating\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv('./data/book_data.csv')\n",
    "    seen_books = set(df['URL_ID'])\n",
    "    book_data = df.values.tolist()\n",
    "except: # Only needed for a first run.\n",
    "    seen_books = set()\n",
    "    book_data = []\n",
    "\n",
    "added = 0\n",
    "for url in urls:\n",
    "    if added >= 100:\n",
    "        write(book_data)\n",
    "        added = 0\n",
    "    cur_genre = url.split(\"/\")[-1]\n",
    "    time.sleep(1)\n",
    "    # new_url = new_releases(url)\n",
    "    new_url = most_read(url)\n",
    "    content = \"\"\n",
    "    try: \n",
    "        response = requests.get(new_url)\n",
    "        content = response.content\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Errorparsing url {new_url}: {e}\")    \n",
    "\n",
    "    if content == \"\":\n",
    "        continue\n",
    "\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "    possible_book_links = soup.find_all('div', {'class': 'coverWrapper'})\n",
    "    print(f'There are {len(possible_book_links)} books under the genre of {cur_genre}...')\n",
    "\n",
    "    for i, link in enumerate(possible_book_links):\n",
    "        clear_output(wait=True)\n",
    "        display(f\"Current Genre: {cur_genre}\")\n",
    "        display(f\"Parsing book {i}/{len(possible_book_links)}...\")\n",
    "        a_bit = link.find('a')\n",
    "        if a_bit is None:\n",
    "            display(f\"No redirect associated with {link}.\")\n",
    "            continue\n",
    "        redirect = a_bit['href']\n",
    "        if redirect.startswith('/book/show'):\n",
    "            url_suffix = redirect.split(\"/\")[-1]\n",
    "            # Skip books we may have seen from previous runs or previous genres\n",
    "            try:\n",
    "                url_identifier = int(url_suffix.split(\"-\")[0])\n",
    "            except:\n",
    "                display(f\"URL Suffix is likely a classic split by a '.' : {url_suffix}\")\n",
    "                try:\n",
    "                    url_identifier = int(url_suffix.split(\".\")[0])\n",
    "                except:\n",
    "                    display(f\"Unsuccessful in second attempt to split url suffix: {url_suffix}\")\n",
    "                    continue\n",
    "\n",
    "            if url_identifier in seen_books:\n",
    "                display(f'Skipping {url_suffix}.')\n",
    "                continue\n",
    "            sub_url = gr_prefix+url_suffix\n",
    "            try:\n",
    "                author, title, genres, blurb, rating = get_book_data(sub_url)\n",
    "            except:\n",
    "                continue\n",
    "            added += 1\n",
    "            book_data.append([url_identifier, author, title, genres, blurb, rating, 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 'Current Genre: christian-historical-fiction'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(book_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write(book_data)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This repopulates the ratings from goodreads books that were parsed from silly.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silly_df = pd.read_csv(\"./data/silly_ratings.csv\")\n",
    "# get_book_data(\"https://www.goodreads.com/book/show/19161852-the-fifth-season\")\n",
    "silly_books = silly_df.values.tolist()\n",
    "uids = [x[0] for x in book_data]\n",
    "# print(uids)\n",
    "for br in silly_books[:10]:\n",
    "    url_identifier = str(br[0])\n",
    "    my_rating = int(br[1])\n",
    "    if url_identifier not in uids:\n",
    "        try:\n",
    "            author, title, genres, blurb, rating = get_book_data(gr_prefix+url_identifier)\n",
    "        except:\n",
    "            print(f'Exception: {url_identifier}')\n",
    "            continue\n",
    "        book_data.append([url_identifier, author, title, genres, blurb, rating, my_rating])\n",
    "    else:\n",
    "        print(book_data[uids.index(url_identifier)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-book-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
