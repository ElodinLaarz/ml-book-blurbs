{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DO NOT Change book_data.csv from this Jupyter Notebook - Caleb of the Past\n",
    "\n",
    "There are no safety mechanisms in here and you might overwrite all the data in the csv... I should probably back this up somewhere. Ah, well... A problem for future Caleb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "\n",
    "from IPython.display import clear_output, display\n",
    "from distinctipy import distinctipy # For distinct cmaps\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import mplcursors\n",
    "matplotlib.use('Qt5Agg')\n",
    "\n",
    "from langdetect import detect\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from gensim.models import Word2Vec, TfidfModel\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# Trying out both.\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def load_data_with_header(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    return df\n",
    "\n",
    "books = load_data_with_header(\"./data/book_data.csv\")\n",
    "books['text'] = books.apply(lambda row: f\"Author: {row['Author']}\\n Genres: {row['Genres']}\\n Blurb: {row['Blurb']}\", axis=1)\n",
    "books = books.convert_dtypes()\n",
    "display(books.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Break books out into groups by rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of bins\n",
    "# bin_width = 25\n",
    "# rating_min = books['Average Rating'].min()\n",
    "# rating_max = books['Average Rating'].max()\n",
    "# bins = np.linspace(rating_min, rating_max)\n",
    "\n",
    "# books['Average Rating'].plot(kind='hist', bins=bins, edgecolor='black', alpha=0.7)\n",
    "# display(rating_min)\n",
    "# display(rating_max)\n",
    "# plt.xlabel('Average Ratings')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.title('Histogram of book ratings')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace Books with the top quartile\n",
    "sorted_books = books.sort_values(by='Average Rating', ascending=False)\n",
    "\n",
    "top_fraction = len(sorted_books) // 4\n",
    "top_books = sorted_books.head(top_fraction)\n",
    "display(top_fraction)\n",
    "top_books"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional: Overwrite the ratings.csv with the values in book_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = [(row['URL_ID'], row['My Rating']) for _, row in books.iterrows() if row['My Rating'] > 0]\n",
    "\n",
    "# df = pd.DataFrame(result, columns=[\"UID\", \"My Rating\"])\n",
    "# df = df.convert_dtypes()\n",
    "# df.to_csv(\"./data/ratings.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_drop = ['the', 'of', 'to', 'and', 'a', 'in', 'is', 'it', 'you', 'that', 'he', 'was', 'for', 'on', 'are', 'with', 'as', 'I', 'his', 'they', 'be', 'at', 'one', 'have', 'this', 'from', 'or', 'had', 'by', 'not', 'word', 'but', 'what', 'some', 'we', 'can', 'out', 'other', 'were', 'all', 'there', 'when', 'up', 'use', 'your', 'how', 'said', 'an', 'each', 'she', 'which', 'do', 'their', 'time', 'if', 'will', 'way', 'about', 'many', 'then', 'them', 'write', 'would', 'like', 'so', 'these', 'her', 'long', 'make', 'thing', 'see', 'him', 'two', 'has', 'look', 'more', 'day', 'could', 'go', 'come', 'did', 'number', 'sound', 'no', 'most', 'people', 'my', 'over', 'know']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO(Break up into better tokens since Japanese doesn't have spaces.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_map = {\n",
    "    'cs': 'czech',\n",
    "    'da': 'danish',\n",
    "    'nl': 'dutch',\n",
    "    'en': 'english',\n",
    "    'et': 'estonian',\n",
    "    'fi': 'finnish',\n",
    "    'fr': 'french',\n",
    "    'de': 'german',\n",
    "    'el': 'greek',\n",
    "    'it': 'italian',\n",
    "    'no': 'norwegian',\n",
    "    'pl': 'polish',\n",
    "    'pt': 'portuguese',\n",
    "    'ru': 'russian',\n",
    "    'sl': 'slovene',\n",
    "    'es': 'spanish',\n",
    "    'sv': 'swedish',\n",
    "    'tr': 'turkish',\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We happen to know that there are manga in the dataset, which has Japanese; so, we allow that to be tokenized as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fugashi\n",
    "\n",
    "# This is our sample text.\n",
    "# \"Fugashi\" is a Japanese snack primarily made of gluten.\n",
    "text = \"麩菓子は、麩を主材料とした日本の菓子。\"\n",
    "\n",
    "# The Tagger object holds state about the dictionary. \n",
    "tagger = fugashi.Tagger()\n",
    "\n",
    "words = [word.surface for word in tagger(text)]\n",
    "print(*words)\n",
    "# => 麩 菓子 は 、 麩 を 主材 料 と し た 日本 の 菓子 。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_langs = set()\n",
    "\n",
    "# The Tagger object holds state about the Japanese dictionary. \n",
    "tagger = fugashi.Tagger()\n",
    "\n",
    "\n",
    "def tokenize_by_lang(l):\n",
    "    tokenizations = []\n",
    "    for text in l:\n",
    "        lang = detect(text)\n",
    "        text = \"\".join([char for char in list(text) if char not in string.punctuation])\n",
    "        text = \" \".join([x.strip().lower() for x in text.split(\" \")])\n",
    "        text = \" \".join([x for x in text.split(\" \") if x not in stop_words and len(x.strip()) != 1 and x not in words_to_drop])\n",
    "        # print(text)\n",
    "        if lang in language_map:\n",
    "            tokenizations.append([x for x in word_tokenize(text, language=language_map[lang]) if x not in stop_words and len(x.strip()) != 1])\n",
    "        elif lang == 'ja':\n",
    "             tokenizations.append([word.surface for word in tagger(text)])\n",
    "        else:\n",
    "            if lang not in failed_langs: # Don't print every time -- just every language.\n",
    "                print(f\"Tried to tokenize: {lang} and failed. Defaulting to English tokenization >.<.\")\n",
    "                failed_langs.add(lang)\n",
    "            tokenizations.append(word_tokenize(text))\n",
    "    return tokenizations\n",
    "\n",
    "print(tokenize_by_lang([\"testing, For for  djiasodjisa823u18rj9201rj   the and know  I one two three the and nope\"]))\n",
    "print(tokenize_by_lang([\"麩菓子は、麩を主材料とした日本の菓子。\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note which df you are generating from! (Might be the top quartile or otherwise.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Easily replace books by other dfs\n",
    "# df_of_interest = books\n",
    "df_of_interest = top_books\n",
    "\n",
    "sentences = tokenize_by_lang(df_of_interest['text'].tolist())\n",
    "embedding_size = 150 # The internet is vague and says \"a number between 100 and 1000 is pretty good or something\"\n",
    "e = 150\n",
    "print(f\"Starting word2vec training for {e} epochs...\")\n",
    "word2vec = Word2Vec(sentences, vector_size=embedding_size, min_count=2, window=7, epochs=e)\n",
    "\n",
    "dictionary = Dictionary(sentences)\n",
    "corpus = [dictionary.doc2bow(sentence) for sentence in sentences]\n",
    "\n",
    "print(f\"Starting to train TfidfModel on the corpus of size {len(corpus)}\")\n",
    "# Train a Gensim TF-IDF model using the corpus\n",
    "tfidf = TfidfModel(corpus)\n",
    "\n",
    "def sentence_embedding(sentence, model, tfidf_model, dictionary):\n",
    "    words = tokenize_by_lang([sentence])[0]\n",
    "    assert(type(words) == list)\n",
    "    word_embeddings = []\n",
    "    \n",
    "    for word in words:\n",
    "        if word == '':\n",
    "            continue\n",
    "        if word in model.wv:\n",
    "            # print(f\"Word '{word}' found in word2vec model\")\n",
    "            bow = dictionary.doc2bow([word])\n",
    "            if len(bow) > 0:\n",
    "                # print(f\"Word '{word}' found in dictionary with ID {bow[0][0]}\")\n",
    "                tfidf_weights = tfidf_model[bow]\n",
    "                if len(tfidf_weights) > 0:\n",
    "                    tfidf_weight = tfidf_weights[0][1]\n",
    "                    word_embeddings.append(model.wv[word] * tfidf_weight)\n",
    "                    # print(f\"Word '{word}' added to embeddings with TF-IDF weight {tfidf_weight}\")\n",
    "                else:\n",
    "                    if word not in [\"blurb\", \"author\", \"genres\", \":\"]:\n",
    "                        print(f\"No TF-IDF weights found for word '{word}'\")\n",
    "            else:\n",
    "                print(f\"Word '{word}' not found in dictionary\")\n",
    "        else:\n",
    "            print(f\"Word '{word}' not found in word2vec model\")\n",
    "    \n",
    "    if len(word_embeddings) > 0:\n",
    "        return np.mean(word_embeddings, axis=0)\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)\n",
    "\n",
    "print(f\"Length of dictionary: {len(dictionary)}\")\n",
    "print(f\"Length of corpus: {len(corpus)}\")\n",
    "embeddings = np.array([sentence_embedding(text, word2vec, tfidf, dictionary) for text in df_of_interest['text']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index needs to be dropped from top books to not have OOR error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster the embeddings\n",
    "# n_clusters = 25\n",
    "# kmeans = KMeans(n_clusters=n_clusters,n_init='auto')\n",
    "# cluster_labels = kmeans.fit_predict(embeddings)\n",
    "\n",
    "# Instantiate the GMM object\n",
    "n_clusters = 25\n",
    "gmm = GaussianMixture(n_components=n_clusters)\n",
    "\n",
    "# Fit the GMM model to the data\n",
    "gmm.fit(embeddings)\n",
    "\n",
    "# Predict the cluster labels for the data\n",
    "cluster_labels = gmm.predict(embeddings)\n",
    "\n",
    "# generate n_clusters many visually distinct colours\n",
    "colors = distinctipy.get_colors(n_clusters)\n",
    "cluster_color_cmap = distinctipy.get_colormap(colors, name='cluster')\n",
    "display(cluster_color_cmap)\n",
    "\n",
    "# ratings always go from 1 to 5.\n",
    "rating_colors = distinctipy.get_colors(5)\n",
    "rating_color_cmap = distinctipy.get_colormap(rating_colors, name='ratings')\n",
    "display(rating_color_cmap)\n",
    "\n",
    "df_of_interest['cluster'] = cluster_labels\n",
    "\n",
    "def closest_words_to_vector(model, vector, n=5):\n",
    "    return model.wv.similar_by_vector(vector, topn=n)\n",
    "\n",
    "cluster_mean_embeddings = []\n",
    "for i in range(n_clusters):\n",
    "    cluster_books = df_of_interest[df_of_interest['cluster'] == i]\n",
    "    mean_embedding = np.mean(embeddings[cluster_books.index], axis=0)\n",
    "    cluster_mean_embeddings.append(mean_embedding)\n",
    "\n",
    "n_closest_words = 4\n",
    "cluster_closest_words = [closest_words_to_vector(word2vec, mean_embedding, n=n_closest_words) for mean_embedding in cluster_mean_embeddings]\n",
    "\n",
    "text_cluster_labels = [\" \".join([word for word, _ in closest_words if len(word.strip().lower()) > 1]) for closest_words in cluster_closest_words]\n",
    "for cl in text_cluster_labels:\n",
    "    print(cl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect ratings from the user\n",
    "n_books_to_rate = 0\n",
    "\n",
    "# Save user ratings using pandas\n",
    "def save_user_ratings(user_ratings):\n",
    "    ratings_df = pd.DataFrame(user_ratings, columns=['UID', 'My Rating'])\n",
    "    ratings_df.to_csv(ratings_file, index=False)\n",
    "\n",
    "# Load user ratings using pandas\n",
    "def load_user_ratings():\n",
    "    user_ratings = {}\n",
    "    if os.path.exists(ratings_file) and os.path.getsize(ratings_file) > 0:\n",
    "        ratings_df = pd.read_csv(ratings_file)\n",
    "        user_ratings = pd.read_csv(\"./data/ratings.csv\")\n",
    "    return user_ratings.values.tolist()\n",
    "\n",
    "# Function to present a profile to the user and collect their rating\n",
    "def present_book_and_rate(df, cluster, rating_uids):\n",
    "    clear_output(wait=True)\n",
    "    while True: \n",
    "        sample = df[df['cluster'] == cluster].sample(1).iloc[0]\n",
    "        if sample['URL_ID'] not in rating_uids:\n",
    "            break\n",
    "    display(sample['Blurb']) # Should I present more than just the blurb?\n",
    "    rating = int(input(\"Please rate this book blurb from 1 to 5: \"))\n",
    "    return sample['URL_ID'], rating\n",
    "\n",
    "ratings_file = './data/ratings.csv'\n",
    "\n",
    "# Load previously saved user ratings\n",
    "user_ratings = load_user_ratings()\n",
    "# print(user_ratings)\n",
    "\n",
    "uids = set()\n",
    "for u in user_ratings:\n",
    "    uids.add(u[0])\n",
    "\n",
    "for i in range(n_books_to_rate):\n",
    "    cluster = np.random.randint(0, n_clusters)\n",
    "    uid, rating = present_book_and_rate(df_of_interest, cluster, uids)\n",
    "    user_ratings.append([uid, rating])\n",
    "\n",
    "# Save the updated user ratings\n",
    "save_user_ratings(user_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the dataset for training the rating prediction model\n",
    "# Update the \"My Rating\" column based on the list of pairs\n",
    "for uid, r in user_ratings:\n",
    "    df_of_interest.loc[df_of_interest['URL_ID'] == uid, 'My Rating'] = r\n",
    "\n",
    "train_data, test_data = train_test_split(df_of_interest[df_of_interest['My Rating'] > 0], test_size=0.3, random_state=42)\n",
    "\n",
    "X_train = embeddings[train_data.index]\n",
    "y_train = train_data['My Rating']\n",
    "\n",
    "X_test = embeddings[test_data.index]\n",
    "y_test = test_data['My Rating']\n",
    "\n",
    "# Train a Ridge regression model to predict user ratings\n",
    "ridge = Ridge(alpha=1.0, random_state=42)\n",
    "ridge.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = ridge.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean squared error: {mse}')\n",
    "\n",
    "# Predict ratings for all profiles in the dataset\n",
    "df_of_interest['predicted_rating'] = ridge.predict(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply t-SNE to reduce the dimensionality to 2D\n",
    "tsne = TSNE(n_components=2)\n",
    "embeddings_2d = tsne.fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The coloring is wrong for some reason-- not coloring by cluster appropriately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.widgets import Button\n",
    "import mplcursors\n",
    "\n",
    "col_to_color = {\"My Rating\": rating_color_cmap, \"cluster\": cluster_color_cmap}\n",
    "\n",
    "def get_text_color(bg_color):\n",
    "    r, g, b, _ = bg_color\n",
    "    luminance = (0.299 * r + 0.587 * g + 0.114 * b)\n",
    "    return 'white' if luminance < 0.5 else 'black'\n",
    "\n",
    "# Function to update the scatter plot\n",
    "def update_scatter(ax, fig, coloring_column: str):\n",
    "    scatter = ax.collections[0]  # Access the scatter object via ax\n",
    "    scatter.set_array(np.array(df_of_interest[coloring_column], dtype=float))\n",
    "    fig.canvas.draw_idle()\n",
    "\n",
    "# Button click event handler\n",
    "def on_click(ax, fig):\n",
    "    global coloring_by_cluster\n",
    "    coloring_by_cluster = not coloring_by_cluster\n",
    "    coloring_column = 'cluster' if coloring_by_cluster else 'My Rating'\n",
    "    update_scatter(ax, fig, coloring_column)\n",
    "    button.label.set_text(f\"Color by {'My Rating' if coloring_by_cluster else 'cluster'}\")\n",
    "    fig.canvas.draw_idle()\n",
    "\n",
    "def plot_embeddings(coloring_column, alpha=0.5):\n",
    "    # Plot the 2D embeddings with labels\n",
    "    plt.rcParams['font.family'] = 'Kiwi Maru'\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    scatter = ax.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], c=df_of_interest[coloring_column].astype(float), cmap=col_to_color[coloring_column], alpha=alpha)\n",
    "    \n",
    "    label_dict = {}\n",
    "    annotation_color = {}\n",
    "    for idx, row in df_of_interest.iterrows():\n",
    "        suffix = \":: \" + str(row['My Rating']) + \"/5\"\n",
    "        if suffix == '0':\n",
    "            suffix = ' unread'\n",
    "            annotation_color[idx] = 'gray'\n",
    "        else:\n",
    "            annotation_color[idx] = plt.get_cmap(col_to_color[coloring_column])(row[coloring_column] / df_of_interest[coloring_column].max())\n",
    "        \n",
    "        label_dict[idx] = str(row['Title'] + suffix)\n",
    "\n",
    "    def label_func(index):\n",
    "        return label_dict.get(index, '')\n",
    "\n",
    "    def annotation_color_func(index):\n",
    "        return annotation_color.get(index, 'black')\n",
    "\n",
    "    # Add interactive labels when hovering over points\n",
    "    cursor = mplcursors.cursor(scatter, hover=True)\n",
    "    cursor.connect('add', lambda sel: (sel.annotation.get_bbox_patch().set(fc=annotation_color_func(sel.index), alpha=1),\n",
    "                                       sel.annotation.set_color(get_text_color(annotation_color_func(sel.index))),\n",
    "                                       sel.annotation.set_text(label_func(sel.index))))\n",
    "    \n",
    "    cluster_and_embeddings_2d = df_of_interest[['cluster']].copy()\n",
    "    cluster_and_embeddings_2d['x'] = embeddings_2d[:, 0]\n",
    "    cluster_and_embeddings_2d['y'] = embeddings_2d[:, 1]\n",
    "    \n",
    "    mean_2d_positions = cluster_and_embeddings_2d.groupby('cluster').mean()\n",
    "    \n",
    "    for label in df_of_interest['cluster'].unique():\n",
    "        color = col_to_color['cluster'](label)  # get color for cluster label\n",
    "        text = text_cluster_labels[label]  # get cluster label text\n",
    "        bbox = dict(facecolor=color, edgecolor='black', boxstyle='round,pad=0.2')  # create bbox with facecolor\n",
    "        ax.annotate(text, mean_2d_positions.loc[label], fontsize=8, ha='center', va='center', bbox=bbox)  # annotate with bbox\n",
    "    # for i, (x, y) in mean_2d_positions.iterrows():\n",
    "    #     ax.annotate(text_cluster_labels[i], (x, y), fontsize=8, ha='center', va='center', bbox=dict(facecolor=cluster_color_cmap, edgecolor='black', boxstyle='round,pad=0.2'))\n",
    "\n",
    "    # Set plot title and axis labels\n",
    "    ax.set_title(f\"2D Visualization of Data with Labels (Colored by {coloring_column})\")\n",
    "    ax.set_xlabel(\"t-SNE Dimension 1\")\n",
    "    ax.set_ylabel(\"t-SNE Dimension 2\")\n",
    "\n",
    "    # Create and position the button\n",
    "    button_ax = plt.axes([0.85, 0.025, 0.12, 0.04])  # x, y, width, height\n",
    "    button = Button(button_ax, f\"Color by {'My Rating' if coloring_by_cluster else 'Cluster'}\")\n",
    "    \n",
    "    # Connect the button to the on_click function\n",
    "    def button_on_click(event):\n",
    "        on_click(ax, fig)\n",
    "\n",
    "    button.on_clicked(button_on_click)\n",
    "    \n",
    "    return button, fig, ax\n",
    "\n",
    "# Initial plot\n",
    "coloring_by_cluster = True\n",
    "if coloring_by_cluster:\n",
    "    button, fig, ax = plot_embeddings('cluster', 0.3)\n",
    "else:\n",
    "    button, fig, ax = plot_embeddings('My Rating', 0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "print(matplotlib.get_cachedir())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-book-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
